{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset consists of a training set of 60000 examples and test set of 10000 examples. All digits have been size-normalized and centered in a fixed image of 28X28 size. In original dataset, each pixel in the image is represented by an integer between 0 and 255, where 0 is black, 255 is white and anything between represents different shade of gray.\n",
    "We will split the training set of 60000 examples into two sets. First set of 50000 randomly sampled examples will be used for training the neural network. The remainder 10000 examples will be used as a validation set to estimate the hyper-parameters of the network (regularization constant Î», number of hidden units).\n",
    "\n",
    "The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255.\n",
    "\n",
    "Since we have 10 possible digits, there are 10 units in the output layer denoted by k and n training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmishra\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from math import sqrt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(n_in, n_out):\n",
    "    \"\"\"\n",
    "    This function returns the random weights for Neural Network given the\n",
    "    number of node in the input layer and output layer\n",
    "\n",
    "    Input:\n",
    "    n_in: number of nodes of the input layer\n",
    "    n_out: number of nodes of the output layer\n",
    "\n",
    "    Output:\n",
    "    W: matrix of random initial weights with size (n_out x (n_in + 1))\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = sqrt(6) / sqrt(n_in + n_out + 1)\n",
    "    W = (np.random.rand(n_out, n_in + 1) * 2 * epsilon) - epsilon\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Notice that z can be a scalar, a vector or a matrix return the sigmoid of input z\n",
    "    \"\"\"\n",
    "    return  1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" \n",
    "    Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains\n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data\n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the\n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains\n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - feature selection\n",
    "     \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "    # Pick a reasonable size for validation data\n",
    "\n",
    "    # ------------Initialize preprocess arrays----------------------#\n",
    "    train_preprocess = np.zeros(shape=(50000, 784))\n",
    "    validation_preprocess = np.zeros(shape=(10000, 784))\n",
    "    test_preprocess = np.zeros(shape=(10000, 784))\n",
    "    train_label_preprocess = np.zeros(shape=(50000,))\n",
    "    validation_label_preprocess = np.zeros(shape=(10000,))\n",
    "    test_label_preprocess = np.zeros(shape=(10000,))\n",
    "    # ------------Initialize flag variables----------------------#\n",
    "    train_len = 0\n",
    "    validation_len = 0\n",
    "    test_len = 0\n",
    "    train_label_len = 0\n",
    "    validation_label_len = 0\n",
    "    # ------------Start to split the data set into 6 arrays-----------#\n",
    "    for key in mat:\n",
    "        # -----------when the set is training set--------------------#\n",
    "        if \"train\" in key:\n",
    "            label = key[-1]  # record the corresponding label\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)  # get the length of current training set\n",
    "            tag_len = tup_len - 1000  # defines the number of examples which will be added into the training set\n",
    "\n",
    "            # ---------------------adding data to training set-------------------------#\n",
    "            train_preprocess[train_len:train_len + tag_len] = tup[tup_perm[1000:], :]\n",
    "            train_len += tag_len\n",
    "\n",
    "            train_label_preprocess[train_label_len:train_label_len + tag_len] = label\n",
    "            train_label_len += tag_len\n",
    "\n",
    "            # ---------------------adding data to validation set-------------------------#\n",
    "            validation_preprocess[validation_len:validation_len + 1000] = tup[tup_perm[0:1000], :]\n",
    "            validation_len += 1000\n",
    "\n",
    "            validation_label_preprocess[validation_label_len:validation_label_len + 1000] = label\n",
    "            validation_label_len += 1000\n",
    "\n",
    "            # ---------------------adding data to test set-------------------------#\n",
    "        elif \"test\" in key:\n",
    "            label = key[-1]\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)\n",
    "            test_label_preprocess[test_len:test_len + tup_len] = label\n",
    "            test_preprocess[test_len:test_len + tup_len] = tup[tup_perm]\n",
    "            test_len += tup_len\n",
    "            # ---------------------Shuffle,double and normalize-------------------------#\n",
    "    train_size = range(train_preprocess.shape[0])\n",
    "    train_perm = np.random.permutation(train_size)\n",
    "    train_data = train_preprocess[train_perm]\n",
    "    train_data = np.double(train_data)\n",
    "    train_data = train_data / 255.0\n",
    "    train_label = train_label_preprocess[train_perm]\n",
    "\n",
    "    validation_size = range(validation_preprocess.shape[0])\n",
    "    vali_perm = np.random.permutation(validation_size)\n",
    "    validation_data = validation_preprocess[vali_perm]\n",
    "    validation_data = np.double(validation_data)\n",
    "    validation_data = validation_data / 255.0\n",
    "    validation_label = validation_label_preprocess[vali_perm]\n",
    "\n",
    "    test_size = range(test_preprocess.shape[0])\n",
    "    test_perm = np.random.permutation(test_size)\n",
    "    test_data = test_preprocess[test_perm]\n",
    "    test_data = np.double(test_data)\n",
    "    test_data = test_data / 255.0\n",
    "    test_label = test_label_preprocess[test_perm]\n",
    "\n",
    "    # Feature selection\n",
    "    selectedFeat = []\n",
    "    rejectedFeat = []\n",
    "    same = []\n",
    "    for i in range(train_preprocess.shape[1]):\n",
    "        topVal = train_data[0,i]\n",
    "        for j in range(train_preprocess.shape[0]):\n",
    "            same.append(train_data[j,i] == topVal)\n",
    "        if (False in same):\n",
    "            selectedFeat.append(i)\n",
    "        else:\n",
    "            rejectedFeat.append(i)\n",
    "        same = []\n",
    "\n",
    "    train_data = np.delete(train_data, rejectedFeat, axis = 1)\n",
    "    validation_data = np.delete(validation_data, rejectedFeat, axis = 1)  \n",
    "    test_data = np.delete(test_data, rejectedFeat, axis = 1)\n",
    "    \n",
    "    print('preprocess done')\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    This function computes the value of objective function (negative log\n",
    "    likelihood error function with regularization) given the parameters\n",
    "    of Neural Networks, thetraining data, their corresponding training\n",
    "    labels and lambda - regularization hyper-parameter.\n",
    "\n",
    "    Input:\n",
    "    params: vector of weights of 2 matrices w1 (weights of connections from\n",
    "            input layer to hidden layer) and w2 (weights of connections from\n",
    "            hidden layer to output layer) where all of the weights are contained\n",
    "            in a single vector.\n",
    "    n_input: number of node in input layer (not include the bias node)\n",
    "    n_hidden: number of node in hidden layer (not include the bias node)\n",
    "    n_class: number of node in output layer (number of classes in\n",
    "             classification problem\n",
    "    training_data: matrix of training data. Each row of this matrix\n",
    "                   represents the feature vector of a particular image\n",
    "    training_label: the vector of truth label of training images. Each entry\n",
    "                    in the vector represents the truth label of its corresponding image.\n",
    "    lambda: regularization hyper-parameter. This value is used for fixing the\n",
    "            overfitting problem.\n",
    "\n",
    "    Output:\n",
    "    obj_val: a scalar value representing value of error function\n",
    "    obj_grad: a SINGLE vector of gradient value of error function\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    reshape 'params' vector into 2 matrices of weight w1 and w2\n",
    "    w1: matrix of weights of connections from input layer to hidden layers.\n",
    "        w1(i, j) represents the weight of connection from unit j in input\n",
    "        layer to unit i in hidden layer.\n",
    "    w2: matrix of weights of connections from hidden layer to output layers.\n",
    "        w2(i, j) represents the weight of connection from unit j in hidden\n",
    "        layer to unit i in output layer.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "    obj_val = 0\n",
    "\n",
    "    # feedforward\n",
    "    n = training_data.shape[0]\n",
    "    X = np.concatenate((training_data, np.tile([1], (training_data.shape[0], 1))), 1)\n",
    "    z = sigmoid(np.dot(X, w1.T))\n",
    "    z = np.concatenate((z, np.tile([1], (z.shape[0], 1))), 1)\n",
    "    o = sigmoid(np.dot(z,w2.T))\n",
    "    \n",
    "    \n",
    "    # 1-of-K coding scheme of input data\n",
    "    y = np_utils.to_categorical(training_label)\n",
    "    \n",
    "    obj_val = obj_val + np.sum(y * np.log(o) + (1.0-y) * np.log(1.0-o))\n",
    "    obj_val = -1*obj_val/n\n",
    "    \n",
    "    # with regularization hyper-parameter\n",
    "    obj_val = obj_val + ((lambdaval/(2*n))*(np.sum(np.square(w1))+np.sum(np.square(w2))))\n",
    "    \n",
    "\n",
    "    delta = o-y\n",
    "    \n",
    "    grad_w2 = np.dot(delta.T,z)\n",
    "    grad_w1 = np.dot(((1-z)*z*(np.dot(delta,w2))).T,X)  \n",
    "    \n",
    "    # Remove zero row\n",
    "    grad_w1 = np.delete(grad_w1, n_hidden,0)\n",
    "\n",
    "    # with regularization hyper-parameter\n",
    "    grad_w2 = (grad_w2 + (lambdaval*w2))*(1/n)\n",
    "\n",
    "    grad_w1 = (grad_w1 + (lambdaval*w1))*(1/n)\n",
    "\n",
    "    obj_grad = np.array([])\n",
    "\n",
    "    obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(w1, w2, data):\n",
    "    \"\"\"\n",
    "    Predicts the label of data given the parameter w1, w2 of Neural Network.\n",
    "    \"\"\" \n",
    "    n = data.shape[0]\n",
    "    X = np.hstack((data,np.ones((n,1))))\n",
    "    a = np.dot(w1,X.T)\n",
    "    z = sigmoid(a)\n",
    "    z = np.vstack((z,np.ones((1,n))))\n",
    "    b = np.dot(w2,z)\n",
    "    o = np.transpose(sigmoid(b))\n",
    "    # take value corresponding with max probability\n",
    "    labels = np.array([])\n",
    "    labels = np.argmax(o,axis=1)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess done\n",
      "\n",
      "Time taken: 0:00:28\n",
      "\n",
      "Training set Accuracy:95.59%\n",
      "\n",
      "Validation set Accuracy:94.82%\n",
      "\n",
      "Test set Accuracy:95.29%\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0\n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "# Ending time.\n",
    "end_time = time.time()\n",
    "\n",
    "# Difference between start and end-times.\n",
    "time_dif = end_time - start_time\n",
    "\n",
    "# Print the time-usage.\n",
    "print(\"\\nTime taken: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "# Reshape nnParams from 1D vector into w1 and w2 matrices\n",
    "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "# Test the computed parameters\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, train_data)\n",
    "accuracy = round(np.mean((predicted_label == train_label).astype(float))*100,2)\n",
    "print('\\nTraining set Accuracy:' + str(accuracy) + '%')\n",
    "\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, validation_data)\n",
    "accuracy = round(np.mean((predicted_label == validation_label).astype(float))*100,2)\n",
    "print('\\nValidation set Accuracy:' + str(accuracy) + '%')\n",
    "\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, test_data)\n",
    "accuracy = round(np.mean((predicted_label == test_label).astype(float))*100,2)\n",
    "print('\\nTest set Accuracy:' + str(accuracy) + '%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
